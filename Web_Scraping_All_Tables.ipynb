{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc2478e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a4f96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of your site\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_sovereign_states_by_date_of_formation#Sortable_list'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6fb357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reach out to the site and get information\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123f835b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Find all tables on the page\n",
    "tables = soup.find_all('table')\n",
    "\n",
    "# Directory to save the CSV files\n",
    "desktop = os.path.join(os.path.expanduser('~'), 'Desktop')\n",
    "output_dir = os.path.join(desktop, 'Scraped_Tables')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Iterate through all tables and save them as CSV\n",
    "for idx, table in enumerate(tables, start=1):  # Start index at 1 for human-friendly numbering\n",
    "    # Extract headers\n",
    "    headers = [th.text.strip() for th in table.find_all('th')]\n",
    "    \n",
    "    # Extract rows\n",
    "    rows = []\n",
    "    for row in table.find_all('tr')[1:]:  # Skip the header row\n",
    "        row_data = [cell.text.strip() for cell in row.find_all('td')]\n",
    "        if len(row_data) == len(headers):  # Ensure data matches header length\n",
    "            rows.append(row_data)\n",
    "    \n",
    "    # Create DataFrame if the table has valid data\n",
    "    if headers and rows:\n",
    "        df = pd.DataFrame(rows, columns=headers)\n",
    "        output_path = os.path.join(output_dir, f'table_{idx}.csv')\n",
    "        df.to_csv(output_path, index=False)\n",
    "        print(f\"Table {idx} saved to {output_path}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa80aa8e",
   "metadata": {},
   "source": [
    "### This script scrapes all tables on the specified webpage and saves them as individual CSV files in a folder named 'Scraped_Tables' on your desktop."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
